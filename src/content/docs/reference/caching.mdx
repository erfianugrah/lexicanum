---
title: "Caching with Cloudflare Workers: When to Use What"
description: Which caching mechanism to use when Workers fetch from different origin types, and architectural patterns for multi-Worker systems
author: Erfi Anugrah
---

import { Aside } from "@astrojs/starlight/components";

How caching behaves when Workers fetch from orange-clouded origins on different zones.

**TL;DR:** `cf` caching options (`cacheTtl`, `cacheEverything`, `cacheTtlByStatus`) are ignored for cross-zone orange-clouded origins. Use Cache API or KV instead.

## Which caching approach to use

```
What's the origin type?
│
├─► Non-CF origin ──────────► fetch() with cf options
│                              • cacheTtl
├─► Same zone ─────────────►   • cacheEverything
│                              • cacheTtlByStatus
│
└─► Cross-zone (orange-clouded)
    │
    └─► cf options are IGNORED, choose:
        │
        ├─► Per-colo OK? ────► Cache API
        │                      • Simple, no infrastructure
        │                      • Different colos = different caches
        │
        └─► Need global? ────► KV (you build the cache layer)
                               • Eventual consistency (~60s)
                               • Requires: key generation, TTL management,
                               │   invalidation, chunking for >25 MB
                               │
                               └─► Concurrent writes? ──► Add Durable Objects
```

| Scenario | Approach | Complexity |
|----------|----------|------------|
| Non-CF or same zone | `fetch()` + `cf` options | Low |
| Cross-zone, simple needs | Cache API | Low |
| Cross-zone, global consistency | KV | **High** |
| Cross-zone, coordinated writes | KV + Durable Objects | **Very High** |

<Aside type="tip" title="Start with Cache API">
If per-colo consistency is acceptable, Cache API is significantly simpler than KV. Only choose KV if you specifically need global cache coherence or metadata-driven purging.
</Aside>

---

## Why cross-zone caching is different

From [How the Cache works](https://developers.cloudflare.com/workers/reference/how-the-cache-works/#fetch):

> "First, `fetch` checks to see if the URL matches a different zone. If it does, it reads through that zone's cache (or Worker). Otherwise, it reads through its own zone's cache, even if the URL is for a non-Cloudflare site."

Requests to cross-zone orange-clouded origins route to that zone's edge, not your zone's cache.

From [Cache using fetch](https://developers.cloudflare.com/workers/examples/cache-using-fetch/):

> "Workers operating on behalf of different zones cannot affect each other's cache. You can only override cache keys when making requests within your own zone... or requests to hosts that are not on Cloudflare. When making a request to another Cloudflare zone (for example, belonging to a different Cloudflare customer), that zone fully controls how its own content is cached within Cloudflare; you cannot override it."

Intentional security boundary - one zone can't manipulate another's cache.

## How caching actually works

Three layers influence caching behavior:

1. **Origin response headers** (`Cache-Control`, `Expires`)
2. **Cloudflare zone settings** (Cache Rules, Edge TTL, Browser TTL)
3. **Worker `cf` options** (`cacheTtl`, `cacheEverything`, `cacheTtlByStatus`)

### Default behavior (no `cf` options, no Cache Rules)

When fetching from **same-zone or non-CF origins** without `cf` options, [default caching](https://developers.cloudflare.com/cache/concepts/default-cache-behavior/) applies:

| Scenario | Cached? | Why |
|----------|---------|-----|
| Static file extension (.js, .css, .png, etc.) | Yes | [Default cacheable extensions](https://developers.cloudflare.com/cache/concepts/default-cache-behavior/#default-cached-file-extensions) |
| HTML, JSON, or other content | **No** | Not in default extension list |
| Non-default type with `Cache-Control: public, max-age=3600` | **No** | Cloudflare caches by extension, not MIME type - need `cf` options |
| Origin returns `Cache-Control: no-store` or `private` | No | Explicitly non-cacheable |
| Response has `Set-Cookie` header | Depends | With `cacheTtl`: cached, cookie removed. With `cacheEverything` alone: not cached, cookie preserved. See [docs](https://developers.cloudflare.com/cache/concepts/cache-behavior/#interaction-of-set-cookie-response-header-with-cache) |

`Cache-Control` headers control *how long* something is cached, not *whether* it gets cached. Non-default types need `cf` options or a Cache Rule.

<Aside type="caution" title="Cross-zone is different">
For cross-zone orange-clouded origins, your `cf` caching options are ignored - the request is routed to the origin zone's edge, bypassing your zone's cache entirely. If the origin Worker generates responses directly (e.g., HTML in code), there's no `CF-Cache-Status` header since it doesn't go through the CDN cache.
</Aside>

### With `cf` options (same zone or non-CF origin)

| cf Option | Effect |
|-----------|--------|
| `cacheEverything: true` | Cache regardless of file extension (respects origin's `Cache-Control` for TTL) |
| `cacheTtl: 3600` | Force cache for 1 hour (implicit `cacheEverything`, ignores origin headers) |
| `cacheTtlByStatus: { "200-299": 3600 }` | Force cache with TTL by status code (implicit `cacheEverything`) |

`cacheTtl` and `cacheTtlByStatus` implicitly enable `cacheEverything`. 

**TTL control:** `cacheEverything` alone respects origin's `Cache-Control` for TTL. `cacheTtl`/`cacheTtlByStatus` override it.

These options only work for same-zone or non-CF origins. Ignored cross-zone.

### Cross-zone behavior

When fetching cross-zone orange-clouded origins:

1. The request goes to the **origin zone's edge**, not yours
2. The origin zone's Cache Rules and settings apply
3. Your `cf` options are ignored
4. If the origin zone has caching enabled, the origin's `Cache-Control` headers control TTL

This means if you want caching, the **origin zone** must configure it via:
- Cache Rules on the origin zone (to enable caching for non-default types)
- A Worker on the origin zone with `cf` options
- For default cacheable extensions, `Cache-Control` headers control TTL

## cf options compatibility

| cf Option | Non-CF / Same Zone | Cross-Zone |
|-----------|-------------------|------------|
| `cacheTtl`, `cacheEverything`, `cacheTtlByStatus` | Yes | **No** |
| `image` | Yes | **Yes** |
| `polish`, `minify`, `mirage` | Yes | **No** (uses origin zone) |

## Workarounds

### Cache API

The Cache API works for cross-zone fetches because **you explicitly store fetched responses in the local data center's cache** after the fetch completes. The cross-zone fetch still occurs on a cache miss, but once cached, subsequent requests in that colo are served from the local cache.

<Aside type="caution" title="Cache API is per-colo">
Cache API stores data in the data center handling the request. The same URL may have different cached content in different data centers. It does NOT "bypass cross-zone restrictions" - it simply stores the response locally after fetching.
</Aside>

**TTL control:** To honor origin's TTL, preserve the `Cache-Control` header. To override, set your own.

```typescript
async function fetchWithCache(request: Request, originUrl: string, ctx: ExecutionContext): Promise<Response> {
  const cache = caches.default;
  const cacheKey = new Request(originUrl, { method: 'GET' });
  
  // Check cache first
  let cached = await cache.match(cacheKey);
  
  if (cached) {
    // Handle cache bypass (e.g., browser refresh)
    const cacheControl = request.headers.get('Cache-Control');
    const shouldBypass = cacheControl?.includes('no-cache');
    
    if (shouldBypass) {
      // Cancel the body stream to avoid resource leaks
      if (cached.body) {
        await cached.body.cancel();
      }
      cached = undefined;
    } else {
      return cached;
    }
  }
  
  // Fetch from origin
  const originResp = await fetch(originUrl);
  
  // Don't cache error responses
  if (!originResp.ok) {
    return originResp;
  }
  
  // Prepare response for caching
  const headers = new Headers(originResp.headers);
  headers.delete('Set-Cookie'); // Cache API rejects responses with Set-Cookie
  
  if (!headers.has('Cache-Control')) {
    headers.set('Cache-Control', 'public, max-age=3600');
  }
  
  const response = new Response(originResp.body, {
    status: originResp.status,
    headers,
  });
  
  // Store in cache using waitUntil (non-blocking, doesn't delay response)
  ctx.waitUntil(cache.put(cacheKey, response.clone()));
  
  return response;
}
```

<Aside type="caution" title="Memory limits with clone()">
Workers have a **128 MB memory limit**. `response.clone()` buffers the entire body into memory. For large responses, use `body.tee()` instead to stream without full buffering. See [Memory limits](#memory-limits-and-cloning) below.
</Aside>

### KV for global consistency

KV provides global replication with eventual consistency (up to 60s). However, using KV as a cache means **building your own caching layer** - you're responsible for cache key generation, TTL management, invalidation, and purging.

#### Cache API vs KV trade-offs

| Aspect | Cache API | KV |
|--------|-----------|-----|
| **Consistency** | Per-colo (different colos may have different content) | Global (eventually consistent across all colos) |
| **TTL management** | Automatic via `Cache-Control` headers | Manual via `expirationTtl` |
| **Invalidation** | `cache.delete()` per-colo only | `KV.delete()` propagates globally |
| **Purge tooling** | Built-in via Cloudflare dashboard/API | Roll your own or use [cache-kv-purger](https://github.com/erfianugrah/cache-kv-purger) |
| **Value size limit** | 512 MB (but cloning limited by 128 MB Worker memory) | 25 MB (chunking required for larger) |

#### When to use KV over Cache API

- **Global cache coherence required** - All users should see the same cached content regardless of colo
- **Programmatic invalidation** - Need to purge specific items globally (not just per-colo)
- **Cross-Worker sharing** - Multiple Workers need to share cached data
- **Metadata-driven purging** - Need to find and purge items by tags/metadata

#### The hidden complexity

KV caching requires building infrastructure that Cache API provides automatically:

1. **Cache key generation** - Consistent keys across requests
2. **TTL refresh** - Extending TTL on cache hits without rewriting the value
3. **Purge mechanisms** - Finding and deleting related items
4. **Chunking for large files** - Splitting files > 25 MB across multiple keys

#### KV caching architecture

When building a KV-based cache, you need these components:

```
Request Flow with KV Cache
==========================

Request ──► Cache Key ──► KV Lookup ──┬──► HIT ──► TTL Check ──┬──► Fresh ──► Return
            Generation     (+ metadata) │                       │
                                        │                       └──► Stale ──► Background
                                        │                                       Refresh
                                        │
                                        └──► MISS ──► Origin Fetch ──► Transform ──► KV Store ──► Return


Components needed:
┌─────────────────────────────────────────────────────────────────────────────────┐
│                                                                                 │
│  1. Cache Key Generation                                                        │
│     └─► Deterministic keys from request params                                  │
│         e.g., "video:sample.mp4:w=1280:h=720:q=high"                           │
│                                                                                 │
│  2. Metadata Storage                                                            │
│     └─► Store headers, content-type, timestamps alongside value                 │
│         KV metadata field (limited to 1KB)                                      │
│                                                                                 │
│  3. TTL Management                                                              │
│     └─► expirationTtl on write                                                  │
│     └─► Track cachedAt for refresh decisions                                    │
│     └─► Background refresh to avoid blocking                                    │
│                                                                                 │
│  4. Chunking (for values > 25 MB)                                               │
│     └─► Split into chunks with manifest                                         │
│     └─► Reassemble on read                                                      │
│                                                                                 │
│  5. Invalidation / Purging                                                      │
│     └─► By exact key                                                            │
│     └─► By prefix/pattern (requires key listing)                                │
│     └─► By metadata tags (requires search)                                      │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

#### Cache versioning for invalidation

Instead of purging individual keys, increment a version number to invalidate all cached content:

```
Cache Versioning
================

                    ┌─────────────────┐
                    │ Version Store   │
                    │ (KV or config)  │
                    │ version = 42    │
                    └────────┬────────┘
                             │
    Request ──► Build Key ───┴──► "v42:video:sample.mp4:w=1280"
                                        │
                    ┌───────────────────┴───────────────────┐
                    │                                       │
                    ▼                                       ▼
              KV Lookup                              Cache Miss
              for v42 key                            (v41 keys exist
                    │                                 but ignored)
                    ▼                                       │
               Return if                                    ▼
               exists                               Fetch & store
                                                    with v42 key

To invalidate: increment version to 43
- All v42 keys become orphaned (expire via TTL)
- No need to list/delete individual keys
- Atomic invalidation across all colos
```

#### Request coalescing

Prevent duplicate origin fetches when multiple requests arrive for the same uncached content:

```
Request Coalescing
==================

Time ──────────────────────────────────────────────────────►

Req A ──► Cache Miss ──► Add to in-flight map ──► Fetch Origin ──► Store ──► Return
                              │                        │              │
Req B ──► Cache Miss ──► In-flight? YES ──► Wait ──────┴──────────────┼──► Clone & Return
                                                                      │
Req C ──► Cache Miss ──► In-flight? YES ──► Wait ─────────────────────┴──► Clone & Return


Implementation:
┌──────────────────────────────────────────────┐
│  const inFlight = new Map<string,            │
│    Promise<Response>>();                     │
│                                              │
│  if (inFlight.has(cacheKey)) {               │
│    return (await inFlight.get(cacheKey))!    │
│      .clone();                               │
│  }                                           │
│                                              │
│  const promise = fetchAndCache(url);         │
│  inFlight.set(cacheKey, promise);            │
│  const response = await promise;             │
│  inFlight.delete(cacheKey);                  │
│  return response;                            │
└──────────────────────────────────────────────┘
```

#### Chunking for large files

Files > 25 MB must be split across multiple KV keys:

```
KV Chunking Pattern
===================

Store (100 MB file):
                                        ┌─────────────────────┐
                                        │ Manifest Key        │
    100 MB ──► Split into ──►           │ "video:large.mp4"   │
              20 MB chunks              │                     │
                    │                   │ {                   │
                    │                   │   totalSize: 100MB, │
                    ├──► Chunk 0 ──►    │   chunkSize: 20MB,  │
                    │    (20 MB)        │   chunks: [         │
                    │                   │     "...chunk:0",   │
                    ├──► Chunk 1 ──►    │     "...chunk:1",   │
                    │    (20 MB)        │     "...chunk:2",   │
                    │                   │     "...chunk:3",   │
                    ├──► Chunk 2 ──►    │     "...chunk:4"    │
                    │    (20 MB)        │   ],                │
                    │                   │   contentType: ...  │
                    ├──► Chunk 3 ──►    │ }                   │
                    │    (20 MB)        └─────────────────────┘
                    │
                    └──► Chunk 4 ──►
                         (20 MB)

Retrieve (Range: bytes=45000000-50000000):
                                        
    Range Request ──► Read Manifest ──► Calculate: chunks 2-3 needed
                                              │
                                              ├──► Fetch chunk 2
                                              │
                                              └──► Fetch chunk 3
                                                        │
                                                        ▼
                                              Slice & stream to client
```

For production implementations of these patterns, see:
- [video-resizer](https://github.com/erfianugrah/video-resizer) - Full implementation with all patterns above
- [Media Transformation Architecture](/reference/media-transformation-architecture/) - Detailed documentation
- [cache-kv-purger](https://github.com/erfianugrah/cache-kv-purger) - CLI for purging by tags/metadata

#### Basic implementation

```typescript
async function fetchWithKV(originUrl: string, env: Env, ctx: ExecutionContext): Promise<Response> {
  const cacheKey = new URL(originUrl).pathname;
  
  // Check KV first - store body as arrayBuffer, metadata separately
  const { value, metadata } = await env.CACHE_KV.getWithMetadata<{
    contentType: string;
    status: number;
    cachedAt: number;
  }>(cacheKey, 'arrayBuffer');
  
  if (value && metadata) {
    // Optional: refresh TTL on hit without rewriting value
    const age = Date.now() - metadata.cachedAt;
    if (age > 1800000) { // 30 min
      ctx.waitUntil(refreshTTL(env, cacheKey, metadata));
    }
    return new Response(value, {
      status: metadata.status,
      headers: { 'Content-Type': metadata.contentType },
    });
  }
  
  // Fetch from origin
  const response = await fetch(originUrl);
  
  if (!response.ok) {
    return response;
  }
  
  // Store in KV: body as value, headers as metadata
  const body = await response.arrayBuffer();
  const contentType = response.headers.get('Content-Type') || 'application/octet-stream';
  
  // Use waitUntil for non-blocking write
  ctx.waitUntil(
    env.CACHE_KV.put(cacheKey, body, { 
      expirationTtl: 3600,
      metadata: { contentType, status: response.status, cachedAt: Date.now() },
    })
  );
  
  return new Response(body, {
    status: response.status,
    headers: { 'Content-Type': contentType },
  });
}

async function refreshTTL(env: Env, key: string, metadata: object): Promise<void> {
  // KV doesn't support TTL refresh without rewriting - must read and write
  const { value } = await env.CACHE_KV.getWithMetadata(key, 'arrayBuffer');
  if (value) {
    await env.CACHE_KV.put(key, value, {
      expirationTtl: 3600,
      metadata: { ...metadata, cachedAt: Date.now() },
    });
  }
}
```

<Aside type="tip" title="Always use waitUntil for KV writes">
KV writes can take time to propagate. Using `ctx.waitUntil()` ensures the write completes without blocking the response. You can also import `waitUntil` directly from `cloudflare:workers` to avoid passing `ctx` through your code.
</Aside>

<Aside type="caution" title="KV is a commitment">
Choosing KV over Cache API means taking on cache infrastructure responsibilities. For simple caching needs where per-colo consistency is acceptable, Cache API is significantly simpler.
</Aside>

## Memory limits and cloning

Workers have a **128 MB memory limit per isolate**. When using `response.clone()` for caching, the entire response body is buffered into memory. For large responses, this can exceed the limit and cause the Worker to fail.

### Mitigations

**Use `tee()` instead of `clone()`** for large responses. `ReadableStream.tee()` creates two streams from one without immediately buffering the entire body:

```typescript
async function fetchLargeWithCache(originUrl: string, ctx: ExecutionContext): Promise<Response> {
  const cache = caches.default;
  const cacheKey = new Request(originUrl, { method: 'GET' });
  
  const cached = await cache.match(cacheKey);
  if (cached) return cached;
  
  const originResp = await fetch(originUrl);
  if (!originResp.ok || !originResp.body) return originResp;
  
  // tee() creates two streams from one - avoids buffering entire body
  const [stream1, stream2] = originResp.body.tee();
  
  const headers = new Headers(originResp.headers);
  headers.delete('Set-Cookie');
  if (!headers.has('Cache-Control')) {
    headers.set('Cache-Control', 'public, max-age=3600');
  }
  
  const responseToCache = new Response(stream1, { status: originResp.status, headers });
  ctx.waitUntil(cache.put(cacheKey, responseToCache));
  
  return new Response(stream2, { status: originResp.status, headers });
}
```

<Aside type="caution" title="tee() still buffers under load">
If one stream is consumed faster than the other, the slower stream's data will be buffered. For very large responses with slow consumers, consider whether caching is the right approach.
</Aside>

### KV value limits and chunking

KV has a **25 MB value limit**. For larger payloads, use a manifest pattern - store chunks separately and reference them:

```typescript
interface ChunkManifest {
  totalSize: number;
  chunkSize: number;
  chunks: string[];  // KV keys for each chunk
  contentType: string;
}

async function storeLargeFile(key: string, data: ArrayBuffer, env: Env, ctx: ExecutionContext): Promise<void> {
  const CHUNK_SIZE = 20 * 1024 * 1024; // 20 MB chunks (under 25 MB limit)
  const chunks: string[] = [];
  
  for (let offset = 0; offset < data.byteLength; offset += CHUNK_SIZE) {
    const chunkKey = `${key}:chunk:${chunks.length}`;
    const chunk = data.slice(offset, offset + CHUNK_SIZE);
    ctx.waitUntil(env.KV.put(chunkKey, chunk, { expirationTtl: 86400 }));
    chunks.push(chunkKey);
  }
  
  const manifest: ChunkManifest = {
    totalSize: data.byteLength,
    chunkSize: CHUNK_SIZE,
    chunks,
    contentType: 'application/octet-stream',
  };
  
  ctx.waitUntil(env.KV.put(key, JSON.stringify(manifest), { expirationTtl: 86400 }));
}
```

For a complete implementation of this pattern, see [Media Transformation Architecture](/reference/media-transformation-architecture/).

## Microservices architecture

In multi-Worker systems (router → origin services), there are two caching approaches:

### Centralized caching (router handles all)

```
Client
  │
  ▼
┌─────────────────────────┐
│  Router Worker          │
│  (handles all caching)  │
└────────────┬────────────┘
             │
       ┌─────┴─────┐
       ▼           │
   [Cache]         │ hit
       │           │
  miss │           │
       ▼           │
   ┌───────┐       │
   │Origins│───────┘
   │ A B C │
   └───────┘
```

**Which caching mechanism to use in the router:**

| Origin Type | Caching Mechanism | Why |
|-------------|-------------------|-----|
| Not proxied (grey-clouded, non-CF) | `fetch()` with `cf` options | Traffic doesn't go through CF proxy |
| Orange-clouded (same zone) | `fetch()` with `cf` options | Same zone, `cf` options work |
| Orange-clouded (cross-zone) | Cache API or KV | `cf` options ignored, must cache explicitly |

| Pros | Cons |
|------|------|
| Single cache management point | Router becomes bottleneck |
| Consistent behavior | Extra hop latency |
| Easier debugging | Tight coupling to origins |
| Centralized circuit breakers | Router must know all origin semantics |

### Distributed caching (origins decide)

```
Client
  │
  ▼
┌───────────────────────┐
│  Router Worker        │
│  (routing only)       │
└───────────┬───────────┘
            │
    ┌───────┼───────┐
    ▼       ▼       ▼
 Origin   Origin   Origin
   A        B        C
   │                 │
   ▼                 ▼
[Cache]           [Cache]
ttl:3600          ttl:60
```

| Pros | Cons |
|------|------|
| Each service owns its strategy | Inconsistent behavior |
| No single point of failure | Harder to debug system-wide |
| Independent deployments | Complex invalidation |
| Better separation of concerns | Duplicate logic |

### Recommendation

**Prefer distributed caching for cross-zone Worker architectures:**

1. **Router stays stateless** - routing logic only
2. **Origins control their own caching** - using `cf` options or Cache Rules (for non-default types)
3. **If edge caching needed**, router uses Cache API (since `cf` options are ignored anyway)

Why this works:
- `cf` options are ignored cross-zone anyway
- Origins know their own caching needs
- Simpler router = fewer failure modes

### Origin-side caching

Origin Workers cache upstream responses with `cf` options (same zone) and signal cacheability to downstream via headers:

```typescript
// Origin Worker - fetches from upstream API and caches at origin's edge
export default {
  async fetch(request: Request): Promise<Response> {
    // Fetch from upstream with cf options (same zone or non-CF = works)
    // cacheTtl implicitly enables cacheEverything (JSON not cached by default)
    const upstream = await fetch('https://api.example.com/data', {
      cf: { cacheTtl: 3600 },
    });
    
    // Worker-generated responses bypass CDN cache
    // Cache-Control tells downstream (router, browser) how long to cache
    return new Response(upstream.body, {
      status: upstream.status,
      headers: {
        'Content-Type': 'application/json',
        'Cache-Control': 'public, max-age=3600',
      },
    });
  },
};
```

**Two levels of caching here:**
1. **CDN cache** (via `cf` options) - caches the upstream API response at this zone's edge
2. **Downstream cache** (via `Cache-Control` header) - tells the calling Worker/browser how long to cache this response

For details on how `Cache-Control` headers interact with Cloudflare, see [Origin Cache Control](https://developers.cloudflare.com/cache/concepts/cache-control/).

<Aside type="tip" title="When to centralize">
Use centralized caching when:
- **Origins are third-party APIs** you don't control (can't set Cache Rules or `cf` options on their zone)
- **Origins return `no-store` or `private`** but you know the data is safe to cache
- **You need to aggregate** responses from multiple origins into a single cached response
- **Origins are grey-clouded** or non-Cloudflare and you want edge caching without changing origin config
</Aside>
