---
title: "Caching with Cloudflare Workers: When to Use What"
description: Which caching mechanism to use when Workers fetch from different origin types, and architectural patterns for multi-Worker systems
author: Erfi Anugrah
---

import { Aside } from "@astrojs/starlight/components";

How caching behaves when Workers fetch from orange-clouded origins on different zones.

**TL;DR:** `cf` caching options (`cacheTtl`, `cacheEverything`, `cacheTtlByStatus`) are ignored for cross-zone orange-clouded origins. Use Cache API or KV instead.

## Which caching approach to use

```d2
direction: right

origin: What's the origin type? {
  shape: diamond
}

non-cf: Non-CF origin
same-zone: Same zone
cross-zone: Cross-zone\n(orange-clouded)

cf-options: fetch() with cf options {
  cacheTtl
  cacheEverything
  cacheTtlByStatus
}

ignored: cf options IGNORED {
  shape: diamond
}

cache-api: Cache API {
  simple: Simple, no infrastructure
  per-colo: Different colos = different caches
}

kv: KV (you build the cache layer) {
  consistency: Eventual consistency (cache TTL)
  requires: "Requires: key generation,\nTTL management, invalidation,\nchunking for >25 MB"
}

do: Add Durable Objects

origin -> non-cf
origin -> same-zone
origin -> cross-zone

non-cf -> cf-options
same-zone -> cf-options

cross-zone -> ignored

ignored -> cache-api: Per-colo OK?
ignored -> kv: Need global?

kv -> do: Concurrent writes?
```

| Scenario | Approach | Effort |
|----------|----------|--------|
| Non-CF or same zone | `fetch()` + `cf` options | Minimal |
| Cross-zone, simple needs | Cache API | Minimal |
| Cross-zone, global consistency | KV | **Significant** |
| Cross-zone, coordinated writes | KV + Durable Objects | **Extensive** |

<Aside type="tip" title="Start with Cache API">
If per-colo consistency is acceptable, Cache API is significantly simpler than KV. Only choose KV if you specifically need global cache coherence or metadata-driven purging.
</Aside>

---

## Why cross-zone caching is different

From [How the Cache works](https://developers.cloudflare.com/workers/reference/how-the-cache-works/#fetch):

> "First, `fetch` checks to see if the URL matches a different zone. If it does, it reads through that zone's cache (or Worker). Otherwise, it reads through its own zone's cache, even if the URL is for a non-Cloudflare site."

Requests to cross-zone orange-clouded origins route to that zone's edge, not your zone's cache.

From [Cache using fetch](https://developers.cloudflare.com/workers/examples/cache-using-fetch/):

> "Workers operating on behalf of different zones cannot affect each other's cache. You can only override cache keys when making requests within your own zone... or requests to hosts that are not on Cloudflare. When making a request to another Cloudflare zone (for example, belonging to a different Cloudflare customer), that zone fully controls how its own content is cached within Cloudflare; you cannot override it."

Intentional security boundary - one zone can't manipulate another's cache.

## How caching actually works

Three layers influence caching behavior:

1. **Origin response headers** (`Cache-Control`, `Expires`)
2. **Cloudflare zone settings** (Cache Rules, Edge TTL, Browser TTL)
3. **Worker `cf` options** (`cacheTtl`, `cacheEverything`, `cacheTtlByStatus`)

### Default behavior (no `cf` options, no Cache Rules)

When fetching from **same-zone or non-CF origins** without `cf` options, [default caching](https://developers.cloudflare.com/cache/concepts/default-cache-behavior/) applies:

| Scenario | Cached? | Why |
|----------|---------|-----|
| Static file extension (.js, .css, .png, etc.) | Yes | [Default cacheable extensions](https://developers.cloudflare.com/cache/concepts/default-cache-behavior/#default-cached-file-extensions) |
| HTML, JSON, or other content | **No** | Not in default extension list |
| Non-default type with `Cache-Control: public, max-age=3600` | **No** | Cloudflare caches by extension, not MIME type - need `cf` options |
| Origin returns `Cache-Control: no-store` or `private` | No | Explicitly non-cacheable |
| Response has `Set-Cookie` header | Depends | With `cacheTtl`: cached, cookie removed. With `cacheEverything` alone: not cached, cookie preserved. See [docs](https://developers.cloudflare.com/cache/concepts/cache-behavior/#interaction-of-set-cookie-response-header-with-cache) |

`Cache-Control` headers control *how long* something is cached, not *whether* it gets cached. Non-default types need `cf` options or a Cache Rule.

<Aside type="caution" title="Cross-zone is different">
For cross-zone orange-clouded origins, your `cf` caching options are ignored - the request is routed to the origin zone's edge, bypassing your zone's cache entirely. If the origin Worker generates responses directly (e.g., HTML in code), there's no `CF-Cache-Status` header since it doesn't go through the CDN cache.
</Aside>

### With `cf` options (same zone or non-CF origin)

| cf Option | Effect |
|-----------|--------|
| `cacheEverything: true` | Cache regardless of file extension (respects origin's `Cache-Control` for TTL) |
| `cacheTtl: 3600` | Force cache for 1 hour (implicit `cacheEverything`, ignores origin headers) |
| `cacheTtlByStatus: { "200-299": 3600 }` | Force cache with TTL by status code (implicit `cacheEverything`) |

`cacheTtl` and `cacheTtlByStatus` implicitly enable `cacheEverything`. 

**TTL control:** `cacheEverything` alone respects origin's `Cache-Control` for TTL. `cacheTtl`/`cacheTtlByStatus` override it.

These options only work for same-zone or non-CF origins. Ignored cross-zone.

### Cross-zone behavior

When fetching cross-zone orange-clouded origins:

1. The request goes to the **origin zone's edge**, not yours
2. The origin zone's Cache Rules and settings apply
3. Your `cf` options are ignored
4. If the origin zone has caching enabled, the origin's `Cache-Control` headers control TTL

This means if you want caching, the **origin zone** must configure it via:
- Cache Rules on the origin zone (to enable caching for non-default types)
- A Worker on the origin zone with `cf` options
- For default cacheable extensions, `Cache-Control` headers control TTL

## cf options compatibility

| cf Option | Non-CF / Same Zone | Cross-Zone |
|-----------|-------------------|------------|
| `cacheTtl`, `cacheEverything`, `cacheTtlByStatus` | Yes | **No** |
| `image` | Yes | **Yes** |
| `polish`, `minify`, `mirage` | Yes | **No** (uses origin zone) |

### Custom cache keys

The `cacheKey` option lets you control what makes two requests "the same" for caching purposes. The value is a **string** that becomes the cache key identifier.

```typescript
// Example: Cache based on normalized URL (strip tracking params)
const url = new URL(request.url);
['utm_source', 'utm_medium', 'utm_campaign', 'fbclid'].forEach(p => url.searchParams.delete(p));
const normalizedKey = url.toString();

const response = await fetch(request, {
  cf: {
    cacheTtl: 3600,
    cacheKey: normalizedKey,
  },
});
```

```typescript
// Example: Separate cache entries by device type
const device = request.headers.get('CF-Device-Type') || 'desktop';
const deviceCategory = device === 'desktop' ? 'desktop' : 'mobile';
const cacheKey = `${request.url}-${deviceCategory}`;

const response = await fetch(request, {
  cf: {
    cacheTtl: 3600,
    cacheKey: cacheKey,
  },
});
```

```typescript
// Example: Include language in cache key
const lang = request.headers.get('Accept-Language')?.split(',')[0] || 'en';
const cacheKey = `${request.url}-${lang}`;

const response = await fetch(request, {
  cf: {
    cacheTtl: 3600,
    cacheKey: cacheKey,
  },
});
```

You can build arbitrarily complex cache keys by constructing the string yourself - include/exclude query params, add headers, cookies, or any request property. [Cache Rules](https://developers.cloudflare.com/cache/how-to/cache-rules/) offer a no-code alternative for the same functionality.

### Tiered Cache and Cache Reserve

Two additional caching features affect how content is stored and retrieved:

**[Tiered Cache](https://developers.cloudflare.com/cache/how-to/tiered-cache/)** reduces origin load by having upper-tier data centers serve as intermediaries. When a lower-tier colo has a cache miss, it checks upper-tier colos before going to origin.

- Works with `fetch()` and `cf` options (same zone/non-CF)
- **Does NOT work with Cache API** - Cache API is per-colo only
- Enabled at the zone level, not per-request

**[Cache Reserve](https://developers.cloudflare.com/cache/advanced-configuration/cache-reserve/)** provides persistent storage for cached content, preventing eviction during traffic spikes or for infrequently accessed content.

- Extends cache retention beyond standard TTL limits
- Useful for large files or content with long TTLs
- Billed based on storage and operations

<Aside type="note" title="When to consider these">
Tiered Cache helps reduce origin load for high-traffic zones. Cache Reserve helps when you need guaranteed cache persistence for specific content. Both are zone-level features and work alongside Worker caching strategies.
</Aside>

## Cross-zone caching solutions

When `cf` options don't work (cross-zone fetches), you have two choices: Cache API for simplicity, or KV for global consistency.

### Cache API

The Cache API lets you work around cross-zone caching limitations by **explicitly storing fetched responses in your zone's local cache**. On cache miss, the cross-zone fetch still occurs normally. But once you `cache.put()` the response, subsequent requests in that colo are served from your local cache - no cross-zone fetch needed.

Since `caches.default` shares the same namespace as your zone's CDN cache, you're effectively populating your zone's cache with content from another zone. The cross-zone restriction (that `cf` options are ignored) still applies to the *fetch*, but you control what happens to the *response*.

<Aside type="note" title="Cache API is per-colo">
Cache API stores data in the data center handling the request. The same URL may have different cached content in different colos. This is why it's a "workaround" rather than a complete solution - you're caching locally per-colo, not globally.
</Aside>

**TTL control:** To honor origin's TTL, preserve the `Cache-Control` header. To override, set your own.

```typescript
async function fetchWithCache(request: Request, originUrl: string, ctx: ExecutionContext): Promise<Response> {
  const cache = caches.default;
  const cacheKey = new Request(originUrl, { method: 'GET' });
  
  // Check cache first
  let cached = await cache.match(cacheKey);
  
  if (cached) {
    // Handle cache bypass (e.g., browser refresh)
    const cacheControl = request.headers.get('Cache-Control');
    const shouldBypass = cacheControl?.includes('no-cache');
    
    if (shouldBypass) {
      // Cancel the body stream to avoid resource leaks
      if (cached.body) {
        await cached.body.cancel();
      }
      cached = undefined;
    } else {
      return cached;
    }
  }
  
  // Fetch from origin
  const originResp = await fetch(originUrl);
  
  // Don't cache error responses
  if (!originResp.ok) {
    return originResp;
  }
  
  // Prepare response for caching
  const headers = new Headers(originResp.headers);
  headers.delete('Set-Cookie'); // Cache API rejects responses with Set-Cookie
  
  if (!headers.has('Cache-Control')) {
    headers.set('Cache-Control', 'public, max-age=3600');
  }
  
  const response = new Response(originResp.body, {
    status: originResp.status,
    headers,
  });
  
  // Store in cache using waitUntil (non-blocking, doesn't delay response)
  ctx.waitUntil(cache.put(cacheKey, response.clone()));
  
  return response;
}
```

#### Handling large responses

Workers have a **128 MB memory limit**. `response.clone()` buffers the entire body into memory. For large responses, use `body.tee()` instead:

```typescript
async function fetchLargeWithCache(originUrl: string, ctx: ExecutionContext): Promise<Response> {
  const cache = caches.default;
  const cacheKey = new Request(originUrl, { method: 'GET' });
  
  const cached = await cache.match(cacheKey);
  if (cached) return cached;
  
  const originResp = await fetch(originUrl);
  if (!originResp.ok || !originResp.body) return originResp;
  
  // tee() creates two streams from one - avoids buffering entire body
  const [stream1, stream2] = originResp.body.tee();
  
  const headers = new Headers(originResp.headers);
  headers.delete('Set-Cookie');
  if (!headers.has('Cache-Control')) {
    headers.set('Cache-Control', 'public, max-age=3600');
  }
  
  const responseToCache = new Response(stream1, { status: originResp.status, headers });
  ctx.waitUntil(cache.put(cacheKey, responseToCache));
  
  return new Response(stream2, { status: originResp.status, headers });
}
```

<Aside type="caution" title="tee() still buffers under load">
If one stream is consumed faster than the other, the slower stream's data will be buffered. For very large responses with slow consumers, consider whether caching is the right approach.
</Aside>

### KV for global consistency

KV provides global replication with eventual consistency (propagation time depends on cache TTL settings). However, using KV as a cache means **building your own caching layer** - you're responsible for cache key generation, TTL management, invalidation, and purging.

#### Cache API vs KV trade-offs

| Aspect | Cache API | KV |
|--------|-----------|-----|
| **Consistency** | Per-colo (different colos may have different content) | Global (eventually consistent across all colos) |
| **TTL management** | Automatic via `Cache-Control` headers | Manual via `expirationTtl` |
| **Invalidation** | `cache.delete()` per-colo only | `KV.delete()` propagates globally |
| **Purge tooling** | Built-in via Cloudflare dashboard/API | Roll your own or use [cache-kv-purger](https://github.com/erfianugrah/cache-kv-purger) |
| **Value size limit** | No hard limit (but cloning limited by 128 MB Worker memory) | 25 MB (chunking required for larger) |

#### When to use KV over Cache API

- **Global cache coherence required** - All users should see the same cached content regardless of colo
- **Programmatic invalidation** - Need to purge specific items globally (not just per-colo)
- **Cross-Worker sharing** - Multiple Workers need to share cached data
- **Metadata-driven purging** - Need to find and purge items by tags/metadata

#### Basic implementation

```typescript
async function fetchWithKV(originUrl: string, env: Env, ctx: ExecutionContext): Promise<Response> {
  const cacheKey = new URL(originUrl).pathname;
  
  // Check KV first - store body as arrayBuffer, metadata separately
  const { value, metadata } = await env.CACHE_KV.getWithMetadata<{
    contentType: string;
    status: number;
    cachedAt: number;
  }>(cacheKey, 'arrayBuffer');
  
  if (value && metadata) {
    // Optional: refresh TTL on hit without rewriting value
    const age = Date.now() - metadata.cachedAt;
    if (age > 1800000) { // 30 min
      ctx.waitUntil(refreshTTL(env, cacheKey, metadata));
    }
    return new Response(value, {
      status: metadata.status,
      headers: { 'Content-Type': metadata.contentType },
    });
  }
  
  // Fetch from origin
  const response = await fetch(originUrl);
  
  if (!response.ok) {
    return response;
  }
  
  // Store in KV: body as value, headers as metadata
  const body = await response.arrayBuffer();
  const contentType = response.headers.get('Content-Type') || 'application/octet-stream';
  
  // Use waitUntil for non-blocking write
  ctx.waitUntil(
    env.CACHE_KV.put(cacheKey, body, { 
      expirationTtl: 3600,
      metadata: { contentType, status: response.status, cachedAt: Date.now() },
    })
  );
  
  return new Response(body, {
    status: response.status,
    headers: { 'Content-Type': contentType },
  });
}

async function refreshTTL(env: Env, key: string, metadata: object): Promise<void> {
  // KV doesn't support TTL refresh without rewriting - must read and write
  const { value } = await env.CACHE_KV.getWithMetadata(key, 'arrayBuffer');
  if (value) {
    await env.CACHE_KV.put(key, value, {
      expirationTtl: 3600,
      metadata: { ...metadata, cachedAt: Date.now() },
    });
  }
}
```

<Aside type="tip" title="Always use waitUntil for KV writes">
KV writes can take time to propagate. Using `ctx.waitUntil()` ensures the write completes without blocking the response. You can also import `waitUntil` directly from `cloudflare:workers` to avoid passing `ctx` through your code.
</Aside>

#### What you're building

KV caching requires implementing what Cache API gives you automatically:

1. **Cache key generation** - Deterministic keys from request params (e.g., `video:sample.mp4:w=1280:h=720`)
2. **Metadata storage** - Headers, content-type, timestamps alongside value (KV metadata limited to 1KB)
3. **TTL management** - `expirationTtl` on write, tracking `cachedAt` for refresh decisions
4. **Chunking** - Splitting files > 25 MB across multiple keys, reassembling on read
5. **Invalidation** - By exact key, prefix/pattern (requires listing), or metadata tags

#### Advanced patterns

<details>
<summary>**Cache versioning** - Atomic invalidation without listing keys</summary>

Instead of purging individual keys, increment a version number:

```d2
direction: down

version-store: Version Store\n(KV or config) {
  version: "version = 42"
}

request: Request
build-key: Build Key
cache-key: "v42:video:sample.mp4:w=1280"

lookup: KV Lookup\nfor v42 key
miss: Cache Miss\n(v41 keys ignored)

return: Return
fetch-store: Fetch & store\nwith v42 key

invalidate: To invalidate {
  step1: Increment version to 43
  step2: All v42 keys become orphaned
  step3: No need to list/delete keys
  step4: Atomic invalidation across colos
}

request -> build-key -> cache-key
cache-key -> lookup: hit
cache-key -> miss: miss
lookup -> return
miss -> fetch-store
```
</details>

<details>
<summary>**Request coalescing** - Prevent duplicate origin fetches</summary>

When multiple requests arrive for the same uncached content:

```typescript
const inFlight = new Map<string, Promise<Response>>();

async function fetchWithCoalescing(cacheKey: string, url: string): Promise<Response> {
  if (inFlight.has(cacheKey)) {
    return (await inFlight.get(cacheKey))!.clone();
  }
  
  const promise = fetchAndCache(url);
  inFlight.set(cacheKey, promise);
  const response = await promise;
  inFlight.delete(cacheKey);
  return response;
}
```
</details>

<details>
<summary>**Chunking** - Files > 25 MB across multiple keys</summary>

KV has a **25 MB value limit**. Split larger files into chunks with a manifest:

```typescript
interface ChunkManifest {
  totalSize: number;
  chunkSize: number;
  chunks: string[];  // KV keys for each chunk
  contentType: string;
}

async function storeLargeFile(key: string, data: ArrayBuffer, env: Env, ctx: ExecutionContext): Promise<void> {
  const CHUNK_SIZE = 20 * 1024 * 1024; // 20 MB chunks (under 25 MB limit)
  const chunks: string[] = [];
  
  for (let offset = 0; offset < data.byteLength; offset += CHUNK_SIZE) {
    const chunkKey = `${key}:chunk:${chunks.length}`;
    const chunk = data.slice(offset, offset + CHUNK_SIZE);
    ctx.waitUntil(env.KV.put(chunkKey, chunk, { expirationTtl: 86400 }));
    chunks.push(chunkKey);
  }
  
  const manifest: ChunkManifest = {
    totalSize: data.byteLength,
    chunkSize: CHUNK_SIZE,
    chunks,
    contentType: 'application/octet-stream',
  };
  
  ctx.waitUntil(env.KV.put(key, JSON.stringify(manifest), { expirationTtl: 86400 }));
}
```

For retrieval with range request support, see [Media Transformation Architecture](/reference/media-transformation-architecture/).
</details>

For production implementations of all patterns, see:
- [video-resizer](https://github.com/erfianugrah/video-resizer) - Full implementation with versioning, coalescing, chunking
- [Media Transformation Architecture](/reference/media-transformation-architecture/) - Detailed documentation
- [cache-kv-purger](https://github.com/erfianugrah/cache-kv-purger) - CLI for purging by tags/metadata

<Aside type="caution" title="KV is a commitment">
Choosing KV over Cache API means taking on cache infrastructure responsibilities. For simple caching needs where per-colo consistency is acceptable, Cache API is the better choice.
</Aside>

---

## Microservices architecture

In multi-Worker systems (router â†’ origin services), there are two caching approaches:

### Centralized caching (router handles all)

```d2
direction: right

client: Client {shape: person}
router: Router Worker (caching)
cache: Cache {shape: cylinder}
origins: Origins {
  a: A
  b: B
  c: C
}

client -> router -> cache: check
cache -> origins: miss
origins -> cache -> router: hit {style.stroke-dash: 3}
```

**Which caching mechanism to use in the router:**

| Origin Type | Caching Mechanism | Why |
|-------------|-------------------|-----|
| Not proxied (grey-clouded, non-CF) | `fetch()` with `cf` options | Traffic doesn't go through CF proxy |
| Orange-clouded (same zone) | `fetch()` with `cf` options | Same zone, `cf` options work |
| Orange-clouded (cross-zone) | Cache API or KV | `cf` options ignored, must cache explicitly |

| Pros | Cons |
|------|------|
| Single cache management point | Router becomes bottleneck |
| Consistent behavior | Extra hop latency |
| Easier debugging | Tight coupling to origins |
| Centralized circuit breakers | Router must know all origin semantics |

### Distributed caching (origins decide)

```d2
direction: right

client: Client {shape: person}
router: Router Worker (routing only)

origins: {
  origin-a: Origin A
  origin-b: Origin B
  origin-c: Origin C
  cache-a: Cache ttl:3600 {shape: cylinder}
  cache-c: Cache ttl:60 {shape: cylinder}
  
  origin-a -> cache-a
  origin-c -> cache-c
}

client -> router -> origins
```

| Pros | Cons |
|------|------|
| Each service owns its strategy | Inconsistent behavior |
| No single point of failure | Harder to debug system-wide |
| Independent deployments | Harder invalidation |
| Better separation of concerns | Duplicate logic |

### Recommendation

**Prefer distributed caching for cross-zone Worker architectures:**

1. **Router stays stateless** - routing logic only
2. **Origins control their own caching** - using `cf` options or Cache Rules (for non-default types)
3. **If edge caching needed**, router uses Cache API (since `cf` options are ignored anyway)

Why this works:
- `cf` options are ignored cross-zone anyway
- Origins know their own caching needs
- Simpler router = fewer failure modes

### Origin-side caching

Origin Workers cache upstream responses with `cf` options (same zone) and signal cacheability to downstream via headers:

```typescript
// Origin Worker - fetches from upstream API and caches at origin's edge
export default {
  async fetch(request: Request): Promise<Response> {
    // Fetch from upstream with cf options (same zone or non-CF = works)
    // cacheTtl implicitly enables cacheEverything (JSON not cached by default)
    const upstream = await fetch('https://api.example.com/data', {
      cf: { cacheTtl: 3600 },
    });
    
    // Worker-generated responses bypass CDN cache
    // Cache-Control tells downstream (router, browser) how long to cache
    return new Response(upstream.body, {
      status: upstream.status,
      headers: {
        'Content-Type': 'application/json',
        'Cache-Control': 'public, max-age=3600',
      },
    });
  },
};
```

**Two levels of caching here:**
1. **CDN cache** (via `cf` options) - caches the upstream API response at this zone's edge
2. **Downstream cache** (via `Cache-Control` header) - tells the calling Worker/browser how long to cache this response

For details on how `Cache-Control` headers interact with Cloudflare, see [Origin Cache Control](https://developers.cloudflare.com/cache/concepts/cache-control/).

<Aside type="tip" title="When to centralize">
Use centralized caching when:
- **Origins are third-party APIs** you don't control (can't set Cache Rules or `cf` options on their zone)
- **Origins return `no-store` or `private`** but you know the data is safe to cache
- **You need to aggregate** responses from multiple origins into a single cached response
- **Origins are grey-clouded** or non-Cloudflare and you want edge caching without changing origin config
</Aside>
